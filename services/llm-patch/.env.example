# LLM-Patch Service Environment Variables
# Copy to .env and customize

# Server Configuration
PORT=8080
NODE_ENV=development
LOG_LEVEL=info

# LLM Provider Configuration
LLM_PROVIDER=stub
# Options: stub, anthropic, openai, local

# Anthropic/Claude Configuration
ANTHROPIC_API_KEY=
ANTHROPIC_MODEL=claude-3-5-sonnet-20240620

# OpenAI Configuration
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4

# Local LLM Configuration (vLLM, TGI, Ollama, etc.)
LOCAL_LLM_URL=http://localhost:8000/v1
LOCAL_LLM_MODEL=codellama-13b-instruct

# Patch Generation Limits
PATCH_MAX_BYTES=200000
MAX_FILES_IN_PATCH=50
LLM_TIMEOUT_MS=30000

# Content Filtering
MAX_CONTEXT_TOKENS=100000
SUPPORTED_FILE_EXTENSIONS=.ts,.js,.tsx,.jsx,.py,.java,.cs,.cpp,.c,.h,.go,.rs,.rb,.php,.sql,.html,.css,.scss,.sass,.less,.json,.xml,.yaml,.yml,.md,.txt

# Developer Context
DEV_ID=dev

# Cache Configuration (Redis URL for production)
ENABLE_RESULT_CACHE=true
CACHE_TTL_SECONDS=300
