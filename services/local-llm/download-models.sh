#!/bin/bash
# Model Download Script for Local LLM Container

echo "ðŸ¤– Setting up Local LLM Models..."

# Wait for Ollama to be ready
until curl -s http://localhost:11434/api/tags > /dev/null; do
  echo "Waiting for Ollama to start..."
  sleep 2
done

# Download recommended models for code generation
echo "ðŸ“¥ Downloading CodeQwen 7B (Recommended for coding)..."
ollama pull codeqwen:7b

echo "ðŸ“¥ Downloading CodeLlama 7B (Alternative)..."
ollama pull codellama:7b

echo "ðŸ“¥ Downloading Llama 3.1 8B (General purpose)..."
ollama pull llama3.1:8b

echo "âœ… Models ready!"
echo "ðŸ”— API available at: http://localhost:11434"

# List available models
echo "ðŸ“‹ Available models:"
ollama list
